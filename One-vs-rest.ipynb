{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feffc103-f6ed-4d0d-a97b-0210dc5b9c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = load_wine()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b87d4c5e-57a6-4a93-af13-496e6c569220",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.weights = None\n",
    "        self.bias = 0\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        n_samples, n_features = X_train.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            for i in range(n_samples):\n",
    "                xi = X_train.iloc[i]\n",
    "                yi = y_train.iloc[i]\n",
    "                linear_output = np.dot(xi, self.weights) + self.bias\n",
    "                predicted_output = 1 if linear_output >= 0 else 0\n",
    "\n",
    "                if predicted_output != yi:\n",
    "                    self.weights += self.learning_rate * (yi - predicted_output) * xi\n",
    "                    self.bias += self.learning_rate * (yi - predicted_output)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(self.predict_proba(X) >= 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32921ab0-6658-4dc6-b13d-a06d2709b515",
   "metadata": {},
   "outputs": [],
   "source": [
    " class OVRClassifier:\n",
    "    def __init__(self, base_estimator_class, **kwargs):\n",
    "        self.base_estimator_class = base_estimator_class\n",
    "        self.kwargs = kwargs\n",
    "        self.classifiers = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "\n",
    "        for class_label in self.classes_:\n",
    "            y_binary = (y == class_label).astype(int)\n",
    "            classifier = self.base_estimator_class(**self.kwargs)\n",
    "            classifier.fit(X, y_binary)\n",
    "\n",
    "            self.classifiers[class_label] = classifier\n",
    "    def predict(self, X):\n",
    "        scores = {}\n",
    "    \n",
    "        for class_label, classifier in self.classifiers.items():\n",
    "            if hasattr(classifier, \"decision_function\"):\n",
    "                score = classifier.decision_function(X)\n",
    "            elif hasattr(classifier, \"predict_proba\"):\n",
    "                proba = classifier.predict_proba(X)\n",
    "                if proba.ndim == 1:\n",
    "                    score = proba\n",
    "                else:\n",
    "                    score = proba[:, 1]\n",
    "            else:\n",
    "                raise ValueError(\"Klasyfikator nie wspiera predict_proba ani decision_function\")\n",
    "    \n",
    "            scores[class_label] = score\n",
    "    \n",
    "        all_scores = np.vstack([scores[class_label] for class_label in self.classes_]).T\n",
    "        predicted_indices = np.argmax(all_scores, axis=1)\n",
    "        return self.classes_[predicted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bfe7bfa-afa1-4f5e-8c52-3d750b2d2fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true, y_pred, num_classes):\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    \n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        cm[true_label, pred_label] += 1\n",
    "    return cm\n",
    "    \n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "def precision(y_true, y_pred, average='macro'):\n",
    "    cm = confusion_matrix(y_true, y_pred, num_classes=len(np.unique(y_true)))\n",
    "    \n",
    "    if average == 'macro':\n",
    "        precisions = []\n",
    "        for i in range(cm.shape[0]):\n",
    "            TP = cm[i, i]\n",
    "            FP = np.sum(cm[:, i]) - TP\n",
    "            precisions.append(TP / (TP + FP) if (TP + FP) != 0 else 0)\n",
    "        return np.mean(precisions)\n",
    "    \n",
    "    elif average == 'micro':\n",
    "        TP = np.sum(np.diagonal(cm))\n",
    "        FP = np.sum(cm) - TP\n",
    "        return TP / (TP + FP)\n",
    "\n",
    "def recall(y_true, y_pred, average='macro'):\n",
    "    cm = confusion_matrix(y_true, y_pred, num_classes=len(np.unique(y_true)))\n",
    "    \n",
    "    if average == 'macro':\n",
    "        recalls = []\n",
    "        for i in range(cm.shape[0]):\n",
    "            TP = cm[i, i]\n",
    "            FN = np.sum(cm[i, :]) - TP\n",
    "            recalls.append(TP / (TP + FN) if (TP + FN) != 0 else 0)\n",
    "        return np.mean(recalls)\n",
    "    \n",
    "    elif average == 'micro':\n",
    "        TP = np.sum(np.diagonal(cm))\n",
    "        FN = np.sum(cm) - TP\n",
    "        return TP / (TP + FN)\n",
    "\n",
    "def f1_score(y_true, y_pred, average='macro'):\n",
    "    prec = precision(y_true, y_pred, average)\n",
    "    rec = recall(y_true, y_pred, average)\n",
    "    \n",
    "    if average == 'macro':\n",
    "        return 2 * (prec * rec) / (prec + rec) if (prec + rec) != 0 else 0\n",
    "    \n",
    "    elif average == 'micro':\n",
    "        return 2 * (prec * rec) / (prec + rec) if (prec + rec) != 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77748d89-0cf9-4b2f-ab72-efd7b7f9f544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron (One-vs-Rest)\n",
      "Confusion matrix:\n",
      " [[14  0  0]\n",
      " [ 0 14  0]\n",
      " [ 0  0  8]]\n",
      "Accuracy: 1.0\n",
      "Precision (macro): 1.0\n",
      "Recall (macro): 1.0\n",
      "F1-score (macro): 1.0\n",
      "Precision (micro): 1.0\n",
      "Recall (micro): 1.0\n",
      "F1-score (micro): 1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ovr_perceptron = OVRClassifier(Perceptron, learning_rate=0.001, max_iter=1000)\n",
    "ovr_perceptron.fit(X_train, y_train)\n",
    "y_pred_perc = ovr_perceptron.predict(X_test)\n",
    "\n",
    "print(\"Perceptron (One-vs-Rest)\")\n",
    "cm_perc = confusion_matrix(y_test, y_pred_perc, num_classes=3)\n",
    "print(\"Confusion matrix:\\n\", cm_perc)\n",
    "print(\"Accuracy:\", accuracy(y_test, y_pred_perc))\n",
    "print(\"Precision (macro):\", precision(y_test, y_pred_perc, average='macro'))\n",
    "print(\"Recall (macro):\", recall(y_test, y_pred_perc, average='macro'))\n",
    "print(\"F1-score (macro):\", f1_score(y_test, y_pred_perc, average='macro'))\n",
    "print(\"Precision (micro):\", precision(y_test, y_pred_perc, average='micro'))\n",
    "print(\"Recall (micro):\", recall(y_test, y_pred_perc, average='micro'))\n",
    "print(\"F1-score (micro):\", f1_score(y_test, y_pred_perc, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "570bfb8b-38bc-453d-81c6-cac60fc093ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC (sklearn OneVsRestClassifier)\n",
      "Confusion matrix:\n",
      " [[14  0  0]\n",
      " [ 0 14  0]\n",
      " [ 0  0  8]]\n",
      "Accuracy: 1.0\n",
      "Precision (macro): 1.0\n",
      "Recall (macro): 1.0\n",
      "F1-score (macro): 1.0\n",
      "Precision (micro): 1.0\n",
      "Recall (micro): 1.0\n",
      "F1-score (micro): 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_ovr = OVRClassifier(SVC, kernel=\"linear\", probability=True)\n",
    "svc_ovr.fit(X_train, y_train)\n",
    "y_pred_svc = svc_ovr.predict(X_test)\n",
    "\n",
    "print(\"SVC (sklearn OneVsRestClassifier)\")\n",
    "cm_svc = confusion_matrix(y_test, y_pred_svc, num_classes=3)\n",
    "print(\"Confusion matrix:\\n\", cm_svc)\n",
    "print(\"Accuracy:\", accuracy(y_test, y_pred_svc))\n",
    "print(\"Precision (macro):\", precision(y_test, y_pred_svc, average='macro'))\n",
    "print(\"Recall (macro):\", recall(y_test, y_pred_svc, average='macro'))\n",
    "print(\"F1-score (macro):\", f1_score(y_test, y_pred_svc, average='macro'))\n",
    "print(\"Precision (micro):\", precision(y_test, y_pred_svc, average='micro'))\n",
    "print(\"Recall (micro):\", recall(y_test, y_pred_svc, average='micro'))\n",
    "print(\"F1-score (micro):\", f1_score(y_test, y_pred_svc, average='micro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
